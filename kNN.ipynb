{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Dataset/Train.csv\")\n",
    "test = pd.read_csv(\"../Dataset/Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import feature_extraction\n",
    "importlib.reload(feature_extraction)\n",
    "from feature_extraction import *\n",
    "\n",
    "y_train, X_train, y_test, X_test = get_counts(train, test, 3, (1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline K-Nearest Neighbours Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(int(np.sqrt(X_train.shape[0]))) #Baseline take Sqrt(no. of rows of X_train)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "mcc = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "\n",
    "print({f'accuracy: {accuracy}'})\n",
    "print({f'sensitivity: {sensitivity}'})\n",
    "print({f'specificity: {specificity}'})\n",
    "print({f'mcc: {mcc}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Hyperparameter Tuning with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "\n",
    "def mcc_scorer(y_test, y_pred):\n",
    "    return matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(50, 201, 10),  # Number of neighbors to use\n",
    "    'weights': ['distance'],  # Weight function used in prediction\n",
    "    'metric': ['euclidean'],  # Distance metric for tree\n",
    "    'p': [1] # Power parameter for the Minkowski metric\n",
    "}\n",
    "\n",
    "tuned_model = KNeighborsClassifier()\n",
    "\n",
    "mcc_scorer = make_scorer(mcc_scorer)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = tuned_model, param_grid = param_grid, cv = 3, scoring = mcc_scorer, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Model After Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "tuned_y_pred = best_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, tuned_y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"False\", \"True\"])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "mcc = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "\n",
    "print({f'accuracy: {accuracy}'})\n",
    "print({f'sensitivity: {sensitivity}'})\n",
    "print({f'specificity: {specificity}'})\n",
    "print({f'mcc: {mcc}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VarianceThreshold for feature selection (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_arr = []\n",
    "sensitivity_arr = []\n",
    "specificity_arr = []\n",
    "acccuracy_arr = []\n",
    "variance_values = np.arange(0.1, 2, 0.1)\n",
    "\n",
    "for val in variance_values:\n",
    "    X_train, X_test = selecting_high_variance_features(X_train, X_test, val)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors = 50 , weights = 'distance', metric = 'euclidean' , p = 1) # put in best parameters here\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    mcc = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "\n",
    "    acccuracy_arr.append(accuracy)\n",
    "    sensitivity_arr.append(sensitivity)\n",
    "    specificity_arr.append(specificity)\n",
    "    mcc_arr.append(mcc)\n",
    "\n",
    "plt.plot(variance_values, acccuracy_arr, label = 'accuracy')\n",
    "plt.plot(variance_values, mcc_arr, label = 'mcc')\n",
    "plt.plot(variance_values, specificity_arr, label = 'specificity')\n",
    "plt.plot(variance_values, sensitivity_arr, label = 'sensitivity')\n",
    "\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best_var_threshold = variance_values[mcc_arr.index(max(mcc_arr))]\n",
    "acc = acccuracy_arr[mcc_arr.index(max(mcc_arr))]\n",
    "sens = sensitivity_arr[mcc_arr.index(max(mcc_arr))]\n",
    "spec = specificity_arr[mcc_arr.index(max(mcc_arr))]\n",
    "mcc_best = max(mcc_arr)\n",
    "print(f'best threshold for variance for feature selection: {best_var_threshold}')\n",
    "print('with corresponding metrics:')\n",
    "print(f'accuracy: {acc}')\n",
    "print(f'sensitivity: {sens}')\n",
    "print(f'specificity: {spec}')\n",
    "print(f'mcc: {mcc_best}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
